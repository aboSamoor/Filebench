\section{Entropy Generator}\label{sec:ent_des}
\subsection{Entropy function}\label{sub:func}
The entropy is quantity that is defined for a set of data to quantify how much random it is.
For any stream of data that is composed of $n$ symbols, its entropy is given by the following
equation:
\begin{equation}\label{eq:ent}
Entropy = -\sum_{i=1}^n P(s_i)\lg P(s_i)
\end{equation}

$P(s_i)$ is the probability that a symbol $s_i$ is generated by the data generator.
To calculate the probability of a symbol in a generated data, can be done by calculating
the experimental probability according to equation \ref{eq:exp}
\begin{equation}\label{eq:exp}
P(s) = \frac{number\, of\, s\, occurences}{size\, of\, data}
\end{equation}

The maximum entropy that can be obtained from a data generated using $n$ symbols is when the
probability distribution of symbols is uniform. That entropy of a uniform distribution can by calculated as following
\begin{align}
Entropy &= -\sum_{i=1}^n P(s_i)\lg P(s_i) \nonumber \\
        &= -\sum_{i=1}^n \frac{1}{n} \lg \frac{1}{n} \nonumber \\
        &= \frac{-1}{n}\sum_{i=1}^n -\lg n \nonumber \\
        &= \lg n 
\end{align}

To decrease the value of entropy, we can imbalance the uniform distribution. Using this method we can reach zero by increasing the probability of the first symbol to 1 and decreasing the
others to zero. However, calculating the $\epsilon_i$ that should be added to subtracted from every $P(s_i)$
to reach a specific entropy value less than $\lg n$ can be hard. To simplify the situation we 
can focus on changing the probability of two symbols at a time.
Our target to generate all values of $x \ni \lg(n-1)<x<\lg n$ just by changing the probability of two symbols.

\begin{align}
Entropy &= -[(P(s_1)+\epsilon)\lg (P(s_1)+\epsilon) + (P(s_2)-\epsilon)\lg (P(s_2)-\epsilon)\nonumber \\
        &\qquad{} +  \sum_{i=3}^n P(s_i)\lg (P(s_i))]\label{eq:imb_ent}\\
 &= -[(\frac{1}{n}+\epsilon)\lg (\frac{1}{n}+\epsilon) + (\frac{1}{n}-\epsilon)\lg (\frac{1}{n}-\epsilon) \nonumber \\
      &\qquad  +  \frac{n-2}{n}\lg (\frac{1}{n})]\label{eq:imb_ent2}
\end{align}


To prove that the entropy as a function of $\epsilon$ equal to any value between $\lg n, \lg (n-1)$, the maximum entropy using $n$, $n-1$ symbols respectively. We notice the following observations:\\
\begin{itemize}
\item Equation \ref{eq:imb_ent2} shows that entropy is a function in one variable, $\epsilon$.
\item Equation \ref{eq:imb_ent2} also shows that the entropy is a continuous function of $\epsilon$ on the interval $\epsilon \in [0,1/n)$ as it is a result of adding and multiplying continuous functions on the same interval.
\item Entropy is equal $\lg n$ when $\epsilon = 0$.
\item Entropy is equal to $\lg(n) -\frac{2}{n}$  when $\epsilon$ reaches $1/n$
\item $\lg(n) - \frac{2}{n} < \lg(n-1) \,\,\, \forall n > 2$ \footnote{Can be verified by visiting\\ 
\url{http://www.wolframalpha.com/input/?i=lg\%28n-1\%29+-+\%28lg\%28n\%29++-+2\%2Fn\%29&a=*FunClash.lg-_*Log2.Log10-}}.
\end{itemize}
Given the above and using the median value theorem the Entropy function spans over the interval $(\lg (n-1), \lg n)$ using subinterval of $\epsilon$ values.
To get the value of $\epsilon$ we can apply any numerical method to find the roots of the equation.

\subsection{Random generator}\label{sub:gen_des}
In \ref{sub:func} we showed that any entropy value can be obtained using a slightly modified uniform distribution.
Now, given that the probability distribution function (PDF) of our symbols is already calculated.
How can we build a data source that generates the data with the given entropy ?
Again we will use a uniform random source to help us.The idea that we will calculate the cumulative distribution
function (CDF) of the symbols table first. Then use the output of a uniform random generator to search for the
corresponding symbol of the random value. Every symbol has different size interval that correspond to its probability.
Because the CDF is an increasing function, the CDF table is increasing also which allow us to use in our search a binary
search algorithm. 
\begin{algorithm}
\caption{Random Generator}
\label{alg:rnd}
\begin{algorithmic}
\STATE Solve the equation to get the value of $\epsilon$
\STATE Calculate PDF
\STATE Calculate CDF
\FOR{$i = 1$ \TO Buffer size} 
\STATE $x$= Random number in $[0,1)$
\STATE index = search in which interval of CDF $x$ lie.
\RETURN SymbolTable[index]
\ENDFOR
\end{algorithmic}
\end{algorithm}


Because the symbol table has constant size, the cost of the binary search is also constant.
This guarantees that the time complexity of our algorithm is linear. However, in \ref{sec:ent_imp} we will show that in practice the 
constant factors of such algorithm is not good enough. Moreover, will will present other different modifications
of the algorithm.


\section{Design Principles}
During the process of designing we tried our best to maintain the following principles. 

\begin{itemize}

\item \textbf{Minimal change} %suggest better name%

The scope of the changes is as minimal as possible. This applies to the size of the patch counted by number of lines and the number of files modified that were modified. 

The files that are modified
\begin{itemize}
\item fileset.c/fileset.h
\item flowop\_library.c
\item parser\_lex.l
\item parser\_gram.y
\end{itemize}
Moreover, we used any already available structure instead of reinventing the wheel.

\item \textbf{Extensibility} \\

\item \textbf{Backward compatibility} \\
The patched Filebench runs all the old workload model files without modification. The patch is triggered only when the data source attribute is specified.
 The patch is surrounded by conditional compilation preprocessors,\verb+CONFIG_ENTROPY_DATA_EXPERIMENTAL+ , that enables the user to switch the functionality on or off at the compilation time.

\item \textbf{Modularity}\\
Any code that do not change the flow of the current Filebench code base, is separated and kept in separate \verb+C+ modules.
 Files added
\begin{itemize}
\item sources.c/sources.h 
\item entropy.c/entropy.h
\end{itemize}

\end{itemize}
